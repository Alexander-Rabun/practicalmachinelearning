---
title: 'Practical Machine Learning Project'
author: "Donald Chambless"
date: "July 26, 2016"
output: html_document
---

```
rm(list=ls())
{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Introduction:  
In this project, we use the data available at 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv to construct a model for the prediction of the "classe" variable, which takes on the values A through E, from the many 
covariates available in this dataset.  Then, upon achievement of a satisfactory model, we use the 20 observations in the dataset provided at 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv, in which the same covariates-but not the classe variable-are available, to make predictions of the 20 expected values of 
the unknown classe variable.  We include a description of the model construction process and the reasoning behind the decisions made and a citation of our expected out of sample error rate.   

The instructions given to the class relative to this project included the following background:  

**_Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices 
are part of the quantified self-movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because 
they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will 
be to use data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways._**  

The caret package expedites the exploration of a large number of models, and I experimented with a number of these, beginning with the simpler options, such as ctree.  However, I was not 
satisfied with the in-sample error rate of any of these simpler options, so I decided to try the computationally-intensive random forest model with cross validation included.  The process I 
followed in doing this is detailed in the following.

## Reading the data:
I downloaded the data from the URLs indicated above and renamed these two files to give them more descriptive names.  Then I created two corresponding dataframes using the read.csv 
function.  The first five covariates described time stamps and other irrelevant information, so I eliminated those from both dataframes.

```{r}
library(caret)
library(randomForest)

# Read the training and testing datasets:
trainingData <- read.csv("Project.training.data.CSV", header = TRUE)
testingData <- read.csv("Project.testing.data.csv", header = TRUE)
dim(trainingData); dim(testingData)

# Get rid of count, people's names and time stamps:
trainingData <- trainingData[ , 6:160]
testingData <- testingData[ , 6:160]
# 159 covariates reduced to 154
#
```

The caret package includes a "nearZeroVar" function that identifies any covariates that provide very little information.  I used that function to eliminate 60 more variables:

```{r}
# Which variables have very little variance?
NZV <- nearZeroVar(trainingData)        # Returns the column numbers of nzv variables
# Remove variables with near zero variance:
trainingData <- trainingData[ , -NZV]
testingData <- testingData[ , -NZV]
# 154 covariates reduced to 94
```

Examination of the remaining covariates revealed that some of them had a large percentage of NA values; I retained only those that had fewer than 50 percent of NAs.  This reduced the number 
of independent variables remaining for model construction to only 53:

```{r}
# Eliminate covariates that are largely NA:
g <- function(x)  {mean(is.na(x)) < .5}     # Keep these
Index <- sapply(trainingData, g)
trainingData <- trainingData[ , Index]
testingData <- testingData[ , Index]
# Now we're down to 53 covariates
```

## Model construction:  
At this point, my data preprocessing was complete, so I was ready to generate the random forest model.  Since some of the simpler models, such as ctree, gave good-but not excellent-in 
sample error rates, I thought that the simplest random forest would probably be adequate, but since the project directions specifically mentioned cross validation, I included three-fold 
cross validation in the model construction.  The generation of this model, as expected, was very time-consuming on my system, so after I obtained the first successful model construction, I 
saved the result to disk so that future execution of my R markdown script would not suffer from this extremely long computational delay:

```{r}
# Read the model from disk if it's there:
if (file.exists("RandomForestModel.RData")) {load("RandomForestModel.Rdata") } else
{
# Create a random tree model with cross validation; this is going to run forever:
RandomForestModel <- train(classe ~ ., data = trainingData, method = "rf",
                trControl = trainControl(method = "cv", number = 3), prox = TRUE)
# Don't let that sucker get away:
save(RandomForestModel, file = "RandomForestModel.Rdata")
}
```

Just out of curiosity, I used the varImp function to determine the most important variables in this model:

```{r}
 varImp(RandomForestModel)
```

At this point, I was ready to determine in-sample performance of the model:  

## Results obtaining on the training data:  

```{r}
# See how the model does on the training data:
trainingPred <- predict(RandomForestModel, trainingData[ , -54])
confusionMatrix(trainingPred, trainingData$classe)
```

It was very gratifying to find that the model achieved perfect accuracy on the 19,622 observations in the training dataframe-an in-sample error rate of zero.  The confusionMatrix report 
indicates that the expected out-of-sample error rate should be no more than 0.02%.  Therefore, I felt that the since the model was performing optimally on the training dataframe, I could expect that 
I would also obtain excellent results with the testing dataframe.  

## Results obtaining on the testing data:  
Finally, I applied the model to the testing dataframe, which contained 20 observations.  Since we were not given the values of the classe variable in this instance, I am not able to 
determine how successful these predictions are; I can only compute and cite them.

```{r}
# Make the final predictions required:
predict(RandomForestModel, testingData[ , -54])
```
